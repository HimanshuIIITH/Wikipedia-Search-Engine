{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "import timeit\n",
    "import subprocess\n",
    "import mwparserfromhell\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import Stemmer\n",
    "import re\n",
    "import math\n",
    "invertedIndex={}\n",
    "MAX_DOC_COUNT=100\n",
    "index_ct=0\n",
    "docid_wordcount={}\n",
    "doc_id=0\n",
    "\n",
    "reg1 = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',re.DOTALL)\n",
    "reg2 = re.compile(r'{\\|(.*?)\\|}',re.DOTALL)\n",
    "reg3 = re.compile(r'{{v?cite(.*?)}}',re.DOTALL)\n",
    "refreg=r'{{v?cite(.*?)}}'\n",
    "reg4 = re.compile(r'[-.,:;_?()\"/\\']',re.DOTALL)\n",
    "reg5 = re.compile(r'\\[\\[file:(.*?)\\]\\]',re.DOTALL)\n",
    "# reg6 = re.compile(r'[\\'~` \\n\\\"_!=@#$%-^*+{\\[}\\]\\|\\\\<>/?]',re.DOTALL)\n",
    "reg6 = re.compile(r'[\\'~` \\n\\\"_!=@#$%\\-^*+{\\[}\\]\\|\\\\<>/?]',re.DOTALL)\n",
    "catRegExp = r'\\[\\[category:(.*?)\\]\\]'\n",
    "infoRegExp = r'{{infobox(.*?)}}'\n",
    "refRegExp = r'== ?references ?==(.*?)=='\n",
    "reg7 = re.compile(infoRegExp,re.DOTALL)\n",
    "reg8 = re.compile(refRegExp,re.DOTALL)\n",
    "reg9 = re.compile(r'{{(.*?)}}',re.DOTALL)\n",
    "reg10 = re.compile(r'<(.*?)>',re.DOTALL)\n",
    "reg11=re.compile(r'\\[\\[(.*?)\\]\\]')\n",
    "reg12=re.compile(catRegExp,re.DOTALL)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = Stemmer.Stemmer('english') #pystemmer used to stem words\n",
    "invertedIndex={}\n",
    "\n",
    "\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = []\n",
    "        self._id=1\n",
    "        self._doc_ct=1\n",
    "        self._raw_token=0\n",
    "#         self._currentPage\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._pages.append((self._values['title'], self._values['text']))\n",
    "\n",
    "            parsePage(self,self._values,self._doc_ct)\n",
    "            self._pages = []\n",
    "            self._id=self._id+1\n",
    "#             self._doc_ct+=1\n",
    "#             if self._doc_ct==5000:\n",
    "#                 doc_ct=1\n",
    "\n",
    "def remove_newline(text):\n",
    "    text=text.replace('\\n',' ')\n",
    "    text=text.strip()\n",
    "    return text\n",
    "\n",
    "def process_digit(w):\n",
    "    if w.isnumeric():\n",
    "        w=w.lstrip('0')\n",
    "    return w\n",
    "\n",
    "def cleantext(tokenlist):\n",
    "    tokentext=' '.join(tokenlist)\n",
    "#     print(tokentext)\n",
    "    tokentext=reg4.sub(' ',tokentext)\n",
    "    tokentext=reg5.sub(' ',tokentext)\n",
    "    tokentext=reg6.sub(' ',tokentext)\n",
    "    tokenlist=tokentext.split()\n",
    "    return tokenlist\n",
    "    \n",
    "# def is_valid(word):\n",
    "#     if word.isalnum() and len(word)>=2:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "    \n",
    "def is_valid(word):\n",
    "\tif word == \"\"  or len(word) < 3:\n",
    "\t\t\treturn False\n",
    "\ttry:\n",
    "\t\tword.encode(encoding='utf-8').decode('ascii')\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\treturn True\n",
    "    \n",
    "    \n",
    "def field_word_count(dictionary):\n",
    "    ct=0\n",
    "    for key,val in dictionary.items():\n",
    "        ct+=val\n",
    "    return ct\n",
    "        \n",
    "    \n",
    "\n",
    "#inserting into global index\n",
    "# global index is a dict of dict of dict\n",
    "#first level dict \n",
    "def insertIntoIndex(dictionary,field,doc_id):\n",
    "    \n",
    "    word_ct=field_word_count(dictionary)\n",
    "    for key,val in dictionary.items():\n",
    "        if key not in invertedIndex:\n",
    "            invertedIndex[key]={}\n",
    "        if doc_id not in invertedIndex[key]:\n",
    "            invertedIndex[key][doc_id]={}\n",
    "            \n",
    "        tf=val/word_ct\n",
    "        tf=round(math.log(1+tf,10),4)\n",
    "        \n",
    "        if field==\"title\":                      #precedence order  title>infobox=category>ref=external link>body\n",
    "            tf=int((.0100+tf)*10000)            # hence weight of title's tfidf is increased followed by others\n",
    "        if field==\"infobox\":\n",
    "            tf=int((.0050+tf)*10000)\n",
    "        if field==\"category\":\n",
    "            tf=int((.0050+tf)*10000)\n",
    "        if field==\"ref\":\n",
    "            tf=int((.0010+tf)*10000)\n",
    "        if field==\"el\":\n",
    "            tf=int((.0010+tf)*10000)\n",
    "        if field==\"body\":\n",
    "            tf=int((.0001+tf)*10000)\n",
    "        \n",
    "        tf_val=[tf,val]\n",
    "        invertedIndex[key][doc_id][field]=tf_val\n",
    "#         invertedIndex[key][doc_id][field]=val\n",
    "\n",
    "\n",
    "\n",
    "# dump global dict content into intermediate inverted_index which later will be merged using merge function\n",
    "def dumpindex():\n",
    "    global index_ct,docid_wordcount,invertedIndex\n",
    "    print(\"dump\",index_ct)\n",
    "    invertedIndexFile=open(\"inverted_index/\"+str(index_ct)+\".txt\",'w')\n",
    "    index_ct=index_ct+1\n",
    "    words=sorted(invertedIndex.keys())\n",
    "    clean_tokens=len(invertedIndex.keys())\n",
    "    \n",
    "#     content=\"\"\n",
    "    \n",
    "    for word in words:\n",
    "        content=\"\"+word\n",
    "        \n",
    "        documentIdDict=invertedIndex[word]\n",
    "        \n",
    "        for doc_id in sorted(documentIdDict.keys()):\n",
    "            content=content+\"#\"+\"id\"+str(doc_id)\n",
    "            word_ct=0\n",
    "            \n",
    "            if \"body\" in invertedIndex[word][doc_id]:\n",
    "                content=content+\"b\"+str(invertedIndex[word][doc_id][\"body\"][0])\n",
    "                word_ct=word_ct+invertedIndex[word][doc_id][\"body\"][1]\n",
    "            \n",
    "            if \"title\" in invertedIndex[word][doc_id]:\n",
    "                content=content+\"t\"+str(invertedIndex[word][doc_id][\"title\"][0])\n",
    "                word_ct=word_ct+invertedIndex[word][doc_id][\"title\"][1]\n",
    "                \n",
    "            if \"category\" in invertedIndex[word][doc_id]:\n",
    "                content=content+\"c\"+str(invertedIndex[word][doc_id][\"category\"][0])\n",
    "                word_ct=word_ct+invertedIndex[word][doc_id][\"category\"][1]\n",
    "                \n",
    "            if \"infobox\" in invertedIndex[word][doc_id]:\n",
    "                content=content+\"i\"+str(invertedIndex[word][doc_id][\"infobox\"][0])\n",
    "                word_ct=word_ct+invertedIndex[word][doc_id][\"infobox\"][1]\n",
    "                \n",
    "            if \"el\" in invertedIndex[word][doc_id]:\n",
    "                content=content+\"e\"+str(invertedIndex[word][doc_id][\"el\"][0])\n",
    "                word_ct=word_ct+invertedIndex[word][doc_id][\"el\"][1]\n",
    "                \n",
    "            if \"ref\" in invertedIndex[word][doc_id]:\n",
    "                content=content+\"r\"+str(invertedIndex[word][doc_id][\"ref\"][0])\n",
    "                word_ct=word_ct+invertedIndex[word][doc_id][\"ref\"][1]\n",
    "#             print(word_ct,docid_wordcount)\n",
    "            tf = word_ct/docid_wordcount[doc_id]\n",
    "            tf = round(math.log(1 + tf, 10), 4)\n",
    "            tf = int((.0001+tf)*10000)\n",
    "            content=content+\"@\"+str(tf)\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "        invertedIndexFile.write(content+'\\n')\n",
    "    invertedIndexFile.close()\n",
    "    invertedIndex={}\n",
    "    return clean_tokens\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# wikipedia page will be parsed here\n",
    "def parsePage(self,page,doc_ct):\n",
    "    global docid_wordcount,invertedIndex\n",
    "    global doc_id\n",
    "    \n",
    "    word_ct=0\n",
    "\n",
    "\n",
    "    title=page['title']\n",
    "#     title=title.lower()\n",
    "    body=page['text']\n",
    "    body=reg1.sub(' ',body)\n",
    "    body=reg2.sub(' ',body)\n",
    "#     body=reg3.sub(' ',body)   process reference first and then place it\n",
    "    body=reg10.sub(' ',body)\n",
    "    body=remove_newline(body)\n",
    "    body=body.lower()\n",
    "    \n",
    "    infodict={}\n",
    "    textdict={}\n",
    "    catdict={}\n",
    "    titledict={}\n",
    "    externalLinkDict={}\n",
    "    refdict={}\n",
    "#     print(doc_id)\n",
    "    \n",
    "\n",
    "        \n",
    "#     processing references\n",
    "    \n",
    "#     if doc_id==12:\n",
    "    references=re.findall(refreg,body,re.DOTALL)\n",
    "\n",
    "\n",
    "    for ref in references:\n",
    "        reftitle=re.findall(r'\\| ?title ?=(.*?)\\|',ref,re.DOTALL) #title is a list here\n",
    "\n",
    "        if(len(reftitle)!=0):\n",
    "            reftitle=reftitle[0]\n",
    "        else:\n",
    "            reftitle=\"\"\n",
    "\n",
    "        reftitletokens=reftitle.split()\n",
    "        reftitletokens=cleantext(reftitletokens)\n",
    "        self._raw_token=self._raw_token+len(reftitletokens)\n",
    "        reftitletokens=[word for word in reftitletokens if word not in stop_words]\n",
    "        reftitletokens=stemmer.stemWords(reftitletokens)\n",
    "        for token in reftitletokens:\n",
    "            if is_valid(token):\n",
    "                token=process_digit(token)\n",
    "                word_ct+=1\n",
    "                if token not in refdict:\n",
    "                    refdict[token]=1\n",
    "                else:\n",
    "                    refdict[token]+=1\n",
    "\n",
    "        \n",
    "    body=reg3.sub(' ',body) #all {{cite}} are already processed so {{cite}} no more required.\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "#     processing infobox\n",
    "    \n",
    "\n",
    "    infobox=re.findall(infoRegExp,body,re.DOTALL)\n",
    "\n",
    "\n",
    "    for info in infobox:\n",
    "        infoTitle=re.findall(r'^(.*?)\\|',info,re.DOTALL)\n",
    "        infotokens=re.findall(r'=(.*?)\\|',info,re.DOTALL)\n",
    "        if len(infoTitle)!=0:\n",
    "            infotokens.append(infoTitle[0])\n",
    "        \n",
    "        infotokens=cleantext(infotokens)\n",
    "        self._raw_token=self._raw_token+len(infotokens)\n",
    "\n",
    "\n",
    "        infotokens=[word for word in infotokens if word not in stop_words]\n",
    "        infotokens=stemmer.stemWords(infotokens)\n",
    "        for token in infotokens:\n",
    "            if is_valid(token):\n",
    "                token=process_digit(token)\n",
    "                word_ct+=1\n",
    "                if token not in infodict:\n",
    "                    infodict[token]=1;\n",
    "                else:\n",
    "                    infodict[token]+=1;\n",
    "                    \n",
    "# processing category\n",
    "    \n",
    "\n",
    "    category=re.findall(catRegExp,body,re.DOTALL)\n",
    "    category=cleantext(category)\n",
    "    self._raw_token=self._raw_token+len(category)\n",
    "    category=[word for word in category if word not in stop_words]\n",
    "    category=stemmer.stemWords(category)\n",
    "    for token in category:\n",
    "        if is_valid(token):\n",
    "            token=process_digit(token)\n",
    "            word_ct+=1\n",
    "            if token not in catdict:\n",
    "                catdict[token]=1\n",
    "            else:\n",
    "                catdict[token]+=1\n",
    "                \n",
    "# processing title\n",
    "    \n",
    "        \n",
    "    title=re.findall(r'[A-Z][a-z]*',title,re.DOTALL)\n",
    "    for i in range(len(title)):\n",
    "        title[i]=title[i].lower()\n",
    "    \n",
    "    self._raw_token=self._raw_token+len(title)\n",
    "    title=stemmer.stemWords(title)\n",
    "\n",
    "    \n",
    "    for token in title:\n",
    "        if is_valid(token):\n",
    "            token=process_digit(token)\n",
    "            word_ct+=1\n",
    "            if token not in titledict:\n",
    "                titledict[token.lower()]=1\n",
    "            else:\n",
    "                titledict[token.lower()]+=1;\n",
    "\n",
    "            \n",
    "#processing external link     \n",
    "    linkcontent=re.findall(r'== ?external links ?==.*',body,re.DOTALL)  #findall returns a list\n",
    "\n",
    "    if len(linkcontent)!=0:\n",
    "        linkcontent=reg11.sub(' ',linkcontent[0])\n",
    "        link=re.findall(r'\\[(.*?)\\]', linkcontent, flags=re.MULTILINE)\n",
    "        link=cleantext(link)\n",
    "        self._raw_token=self._raw_token+len(link)\n",
    "        link=[word for word in link if word not in stop_words]\n",
    "        link=stemmer.stemWords(link)\n",
    "        for l in link:\n",
    "            if is_valid(l):\n",
    "                l=process_digit(l)\n",
    "                word_ct+=1\n",
    "                if l not in externalLinkDict:\n",
    "                    externalLinkDict[l]=1\n",
    "                else:\n",
    "                     externalLinkDict[l]+=1\n",
    "                    \n",
    "    \n",
    "    \n",
    "#processing text box\n",
    "                    \n",
    "    body=reg4.sub(' ',body)\n",
    "    body=reg5.sub(' ',body)\n",
    "    body=reg6.sub(' ',body)\n",
    "    body=reg7.sub(' ',body)\n",
    "    body=reg9.sub(' ',body)\n",
    "    body=reg11.sub(' ',body)\n",
    "    body=reg12.sub(' ',body)\n",
    "    \n",
    "   \n",
    "    bodytokens=cleantext(body.split())\n",
    "    self._raw_token=self._raw_token+len(bodytokens)\n",
    "\n",
    "    bodytokens=[word for word in bodytokens if word not in stop_words]\n",
    "    bodytokens=stemmer.stemWords(bodytokens)\n",
    "\n",
    "    for token in bodytokens:\n",
    "        if is_valid(token):\n",
    "            token=process_digit(token)\n",
    "            word_ct+=1\n",
    "            if token not in textdict:\n",
    "                textdict[token]=1\n",
    "            else:\n",
    "                textdict[token]+=1\n",
    "                \n",
    "                \n",
    "    docid_wordcount[doc_id]=word_ct\n",
    "\n",
    "    insertIntoIndex(infodict,\"infobox\",doc_id)\n",
    "    insertIntoIndex(catdict,\"category\",doc_id)\n",
    "    insertIntoIndex(titledict,\"title\",doc_id)\n",
    "    insertIntoIndex(externalLinkDict,\"el\",doc_id)\n",
    "    insertIntoIndex(textdict,\"body\",doc_id)\n",
    "    insertIntoIndex(refdict,\"ref\",doc_id)\n",
    "    \n",
    "    \n",
    "    if(self._doc_ct==30000):\n",
    "#         print(\"dump_______________________________----\",doc_id)\n",
    "        self._doc_ct=0\n",
    "        dumpindex()\n",
    "        invertedIndex={}\n",
    "        \n",
    "    self._doc_ct=self._doc_ct+1\n",
    "    doc_id=doc_id+1\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "\n",
    "def field_idf_for_word(doc_list):\n",
    "    total_doc=8796395\n",
    "    t_ct=1\n",
    "    c_ct=1\n",
    "    i_ct=1\n",
    "    r_ct=1\n",
    "    e_ct=1\n",
    "    b_ct=1\n",
    "    for doc in doc_list:\n",
    "        posting_t=re.findall(r't[0-9]*',doc,re.DOTALL)\n",
    "        posting_c=re.findall(r'c[0-9]*',doc,re.DOTALL)\n",
    "        posting_i=re.findall(r'i[0-9]+',doc,re.DOTALL)   #otherwise it might match with id123\n",
    "        posting_r=re.findall(r'r[0-9]*',doc,re.DOTALL)\n",
    "        posting_e=re.findall(r'e[0-9]*',doc,re.DOTALL)\n",
    "        posting_b=re.findall(r'b[0-9]*',doc,re.DOTALL)\n",
    "        \n",
    "        if(len(posting_t)):\n",
    "            t_ct+=1\n",
    "        if(len(posting_c)):\n",
    "            c_ct+=1\n",
    "        if(len(posting_i)):\n",
    "            i_ct+=1\n",
    "        if(len(posting_r)):\n",
    "            r_ct+=1\n",
    "        if(len(posting_e)):\n",
    "            e_ct+=1\n",
    "        if(len(posting_b)):\n",
    "            b_ct+=1\n",
    "    \n",
    "    idf_t=total_doc/t_ct\n",
    "    idf_t=round(math.log(1+idf_t,10),3)\n",
    "    \n",
    "    idf_c=total_doc/c_ct\n",
    "    idf_c=round(math.log(1+idf_c,10),3)\n",
    "    \n",
    "    idf_i=total_doc/i_ct\n",
    "    idf_i=round(math.log(1+idf_i,10),3)\n",
    "    \n",
    "    idf_r=total_doc/r_ct\n",
    "    idf_r=round(math.log(1+idf_r,10),3)\n",
    "    \n",
    "    idf_e=total_doc/e_ct\n",
    "    idf_e=round(math.log(1+idf_e,10),3)\n",
    "    \n",
    "    idf_b=total_doc/b_ct\n",
    "    idf_b=round(math.log(1+idf_b,10),3)\n",
    "    \n",
    "    idf_res=[idf_t,idf_c,idf_i,idf_r,idf_e,idf_b]\n",
    "    \n",
    "    return idf_res\n",
    "\n",
    "\n",
    "def tfidf_posting(tf_posting,field_idf_list):\n",
    "    tfidf_pos=\"\"\n",
    "    \n",
    "    \n",
    "    docid=re.findall(r'id[0-9]*',tf_posting,re.DOTALL)\n",
    "#     docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "    tfidf_pos+=docid[0]       # doc id added\n",
    "    \n",
    "    t_pos=re.findall(r't[0-9]*',tf_posting,re.DOTALL)\n",
    "    if(len(t_pos)):\n",
    "        t_tf=int(re.findall(r'[0-9]+',t_pos[0],re.DOTALL)[0])\n",
    "        tfidf=int(math.floor(t_tf*field_idf_list[0]))\n",
    "        tfidf_pos+=\"t\"+str(tfidf)\n",
    "        \n",
    "    c_pos=re.findall(r'c[0-9]*',tf_posting,re.DOTALL)\n",
    "    if(len(c_pos)):\n",
    "        c_tf=int(re.findall(r'[0-9]+',c_pos[0],re.DOTALL)[0])\n",
    "        tfidf=int(math.floor(c_tf*field_idf_list[1]))\n",
    "        tfidf_pos+=\"c\"+str(tfidf)\n",
    "        \n",
    "    i_pos=re.findall(r'i[0-9]+',tf_posting,re.DOTALL)\n",
    "    if(len(i_pos)):\n",
    "        i_tf=int(re.findall(r'[0-9]+',i_pos[0],re.DOTALL)[0])\n",
    "        tfidf=int(math.floor(i_tf*field_idf_list[2]))\n",
    "        tfidf_pos+=\"i\"+str(tfidf)\n",
    "        \n",
    "    r_pos=re.findall(r'r[0-9]*',tf_posting,re.DOTALL)\n",
    "    if(len(r_pos)):\n",
    "        r_tf=int(re.findall(r'[0-9]+',r_pos[0],re.DOTALL)[0])\n",
    "        tfidf=int(math.floor(r_tf*field_idf_list[3]))\n",
    "        tfidf_pos+=\"r\"+str(tfidf)\n",
    "    \n",
    "    e_pos=re.findall(r'e[0-9]*',tf_posting,re.DOTALL)\n",
    "    if(len(e_pos)):\n",
    "        e_tf=int(re.findall(r'[0-9]+',e_pos[0],re.DOTALL)[0])\n",
    "        tfidf=int(math.floor(e_tf*field_idf_list[4]))\n",
    "        tfidf_pos+=\"e\"+str(tfidf)\n",
    "    \n",
    "    b_pos=re.findall(r'b[0-9]*',tf_posting,re.DOTALL)\n",
    "    if(len(b_pos)):\n",
    "        b_tf=int(re.findall(r'[0-9]+',b_pos[0],re.DOTALL)[0])\n",
    "        tfidf=int(math.floor(b_tf*field_idf_list[5]))\n",
    "        tfidf_pos+=\"b\"+str(tfidf)\n",
    "        \n",
    "        \n",
    "    return tfidf_pos\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def merge(index_path,index_count):    #merge seperatly build index file\n",
    "    print(index_count,\" index_count\")\n",
    "    word_limit=0\n",
    "    word_limit_max=40000\n",
    "    final_index_count=0\n",
    "    total_docs=8796395\n",
    "    token_count=0\n",
    "#     total_docs=2\n",
    "    \n",
    "    merged_index=index_path+\"/merged_path\"\n",
    "    \n",
    "    if not os.path.exists(merged_index):\n",
    "        os.makedirs(merged_index)\n",
    "        \n",
    "    final_fp=open(merged_index+\"/\"+str(final_index_count)+\".txt\",'w')\n",
    "    secondary_idx = open( merged_index+ \"/\" + \"secondary_index.txt\",'w')\n",
    "        \n",
    "    heap_list=[]\n",
    "    file_pointers=[]\n",
    "    \n",
    "    i=0\n",
    "    while(i<index_count):\n",
    "#         print(i,\"file pointer update\")\n",
    "        file_pointers.append(open(index_path+\"/\"+str(i)+\".txt\"))\n",
    "        i+=1\n",
    "        \n",
    "    i=0 #index file no from which word is considered\n",
    "    while(i<index_count):\n",
    "#         print(i,\"update heap\")\n",
    "        line=file_pointers[i].readline()\n",
    "        word=line[:line.index('#')]\n",
    "        posting=line[line.index('#')+1:]\n",
    "        tpl=(word,i,posting)\n",
    "        heap_list.append(tpl)\n",
    "        token_count+=1\n",
    "        i+=1\n",
    "        \n",
    "#     print()\n",
    "        \n",
    "    heapq.heapify(heap_list)\n",
    "    \n",
    "    pre_touple=heapq.heappop(heap_list)\n",
    "    pre_word=pre_touple[0]\n",
    "    pre_idx=pre_touple[1]\n",
    "    pre_posting=pre_touple[2]\n",
    "    pre_posting[:-1]\n",
    "    line=file_pointers[pre_idx].readline()\n",
    "    if line!=\"\":\n",
    "        word=line[:line.index('#')]\n",
    "        print()\n",
    "        posting=line[line.index('#')+1:]\n",
    "        tpl=(word,pre_idx,posting)\n",
    "        heap_list.append(tpl)\n",
    "        heapq.heapify(heap_list)\n",
    "        \n",
    "    \n",
    "    while(len(heap_list)>0):           #merge using a heap\n",
    "        token_count+=1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        cur_tpl=heapq.heappop(heap_list)\n",
    "\n",
    "        line=file_pointers[cur_tpl[1]].readline()\n",
    "        \n",
    "        if line!=\"\":\n",
    "\n",
    "            word=line[:line.index('#')]\n",
    "\n",
    "            posting=line[line.index('#')+1:]\n",
    "            tpl=(word,cur_tpl[1],posting)\n",
    "            heap_list.append(tpl)\n",
    "            heapq.heapify(heap_list)\n",
    "        \n",
    "        if(pre_word==cur_tpl[0]):                    #note that no writing happning here so no need to convert tf posting to tfidf posting\n",
    "            pre_idx=cur_tpl[1]\n",
    "          \n",
    "            pre_posting=pre_posting+\"#\"+cur_tpl[2]\n",
    "            pre_posting=pre_posting[:-1]\n",
    "        else:                                       #working on this\n",
    "            docs=pre_posting.split(\"#\")\n",
    "            \n",
    "\n",
    "            idf = total_docs/len(docs)              #overall idf\n",
    "            idf = round(math.log(1 + idf, 10), 3)\n",
    "            \n",
    "            line_to_write=pre_word                  #word to be written \n",
    "            \n",
    "            field_idf_list=field_idf_for_word(docs)   # get field tfidf list here index 0=t,1=c,2=i,3=r,4=e,5=b\n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "            for doc in docs:                        #modifying each doc's field tf to tfidf and writing to merged file\n",
    "                info=doc.split(\"@\")\n",
    "                tf=int(info[1])\n",
    "                tfidf = int(math.floor(tf*idf))    #overall tfidf of word in some document\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                tf_posting_section=info[0]\n",
    "                tfidf_posting_section= tfidf_posting(tf_posting_section,field_idf_list) #convert tf posting  part to tfidf posting part\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "#                 line_to_write+=\"#\"+info[0]+\"@\"+str(tfidf)  #v.0\n",
    "                line_to_write+=\"#\"+tfidf_posting_section+\"@\"+str(tfidf) #v.1\n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "                \n",
    "            if(line_to_write!=pre_word):\n",
    "                final_fp.write(line_to_write+'\\n')\n",
    "                \n",
    "            if word_limit==0:   #first time entry after write happpen so write the first word into secondary index, each merged file is having a word limit\n",
    "                secondary_idx.write(pre_word+'\\n')\n",
    "                \n",
    "            word_limit+=1\n",
    "\n",
    "                \n",
    "            if word_limit==word_limit_max: # if word limit reach word_limit_max then open new file\n",
    "\n",
    "                word_limit=0\n",
    "                final_index_count+=1\n",
    "                final_fp=open(merged_index+\"/\"+str(final_index_count)+\".txt\",'w')\n",
    "                \n",
    "            pre_touple=cur_tpl\n",
    "            pre_word=pre_touple[0]\n",
    "            pre_idx=pre_touple[1]\n",
    "#             print(pre_idx,\"pre idx else\")\n",
    "            pre_posting=pre_touple[2]\n",
    "            pre_posting=pre_posting[:-1]\n",
    "        \n",
    "    return token_count\n",
    "        \n",
    "\n",
    "            \n",
    "#             if(len(heap_list)==0):\n",
    "                \n",
    "        \n",
    "            \n",
    "#     del_index(index_path,index_count)   #if you want to delete intermediate inverted index then uncomment this\n",
    "    \n",
    "                \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def del_index(index_path,index_file_ct):     \n",
    "    i=0\n",
    "    while i<index_file_ct:\n",
    "        file_delete=index_path+\"/\"+str(i)+\".txt\"\n",
    "        if os.path.exists(file_delete):\n",
    "            os.remove(file_delete)\n",
    "        i=i+1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwiki-20200801-pages-articles-multistream1.xml-p1p30303.bz2\n",
      "dump 0\n",
      "documents processed 19797\n",
      "enwiki-20200801-pages-articles-multistream10.xml-p2336423p3046512.bz2\n",
      "dump 1\n",
      "dump 2\n",
      "dump 3\n",
      "dump 4\n",
      "dump 5\n",
      "dump 6\n",
      "dump 7\n",
      "dump 8\n",
      "dump 9\n",
      "dump 10\n",
      "dump 11\n",
      "documents processed 328244\n",
      "enwiki-20200801-pages-articles-multistream11.xml-p3046513p3926861.bz2\n",
      "dump 12\n",
      "dump 13\n",
      "dump 14\n",
      "dump 15\n",
      "dump 16\n",
      "dump 17\n",
      "dump 18\n",
      "dump 19\n",
      "dump 20\n",
      "dump 21\n",
      "dump 22\n",
      "dump 23\n",
      "documents processed 664651\n",
      "enwiki-20200801-pages-articles-multistream12.xml-p3926862p5040436.bz2.part\n",
      "dump 24\n",
      "dump 25\n",
      "dump 26\n",
      "dump 27\n",
      "dump 28\n",
      "dump 29\n",
      "dump 30\n",
      "documents processed 863751\n",
      "enwiki-20200801-pages-articles-multistream13.xml-p5040437p6197594.bz2.part\n",
      "dump 31\n",
      "dump 32\n",
      "dump 33\n",
      "dump 34\n",
      "dump 35\n",
      "dump 36\n",
      "dump 37\n",
      "dump 38\n",
      "documents processed 1089351\n",
      "enwiki-20200801-pages-articles-multistream14.xml-p6197595p7697594.bz2.part\n",
      "dump 39\n",
      "dump 40\n",
      "dump 41\n",
      "dump 42\n",
      "dump 43\n",
      "dump 44\n",
      "dump 45\n",
      "dump 46\n",
      "documents processed 1313251\n",
      "enwiki-20200801-pages-articles-multistream14.xml-p7697595p7744800.bz2\n",
      "dump 47\n",
      "documents processed 1325641\n",
      "enwiki-20200801-pages-articles-multistream15.xml-p7744801p9244800.bz2.part\n",
      "dump 48\n",
      "dump 49\n",
      "dump 50\n",
      "dump 51\n",
      "dump 52\n",
      "dump 53\n",
      "documents processed 1480741\n",
      "enwiki-20200801-pages-articles-multistream15.xml-p9244801p9518048.bz2\n",
      "dump 54\n",
      "dump 55\n",
      "dump 56\n",
      "documents processed 1551306\n",
      "enwiki-20200801-pages-articles-multistream16.xml-p11018049p11539266.bz2\n",
      "dump 57\n",
      "dump 58\n",
      "dump 59\n",
      "dump 60\n",
      "dump 61\n",
      "documents processed 1686097\n",
      "enwiki-20200801-pages-articles-multistream16.xml-p9518049p11018048.bz2\n",
      "dump 62\n",
      "dump 63\n",
      "dump 64\n",
      "dump 65\n",
      "dump 66\n",
      "dump 67\n",
      "dump 68\n",
      "dump 69\n",
      "dump 70\n",
      "dump 71\n",
      "dump 72\n",
      "dump 73\n",
      "documents processed 2033989\n",
      "enwiki-20200801-pages-articles-multistream17.xml-p11539267p13039266.bz2\n",
      "dump 74\n",
      "dump 75\n",
      "dump 76\n",
      "dump 77\n",
      "dump 78\n",
      "dump 79\n",
      "dump 80\n",
      "dump 81\n",
      "dump 82\n",
      "dump 83\n",
      "dump 84\n",
      "dump 85\n",
      "dump 86\n",
      "dump 87\n",
      "dump 88\n",
      "dump 89\n",
      "documents processed 2506734\n",
      "enwiki-20200801-pages-articles-multistream17.xml-p13039267p13693073.bz2\n",
      "dump 90\n",
      "dump 91\n",
      "dump 92\n",
      "dump 93\n",
      "dump 94\n",
      "dump 95\n",
      "dump 96\n",
      "documents processed 2698342\n",
      "enwiki-20200801-pages-articles-multistream18.xml-p13693074p15193073.bz2\n",
      "dump 97\n",
      "dump 98\n",
      "dump 99\n",
      "dump 100\n",
      "dump 101\n",
      "dump 102\n",
      "dump 103\n",
      "dump 104\n",
      "dump 105\n",
      "dump 106\n",
      "dump 107\n",
      "dump 108\n",
      "dump 109\n",
      "dump 110\n",
      "dump 111\n",
      "dump 112\n",
      "documents processed 3156838\n",
      "enwiki-20200801-pages-articles-multistream18.xml-p15193074p16120542.bz2\n",
      "dump 113\n",
      "dump 114\n",
      "dump 115\n",
      "dump 116\n",
      "dump 117\n",
      "dump 118\n",
      "dump 119\n",
      "dump 120\n",
      "dump 121\n",
      "documents processed 3426486\n",
      "enwiki-20200801-pages-articles-multistream19.xml-p16120543p17620542.bz2\n",
      "dump 122\n",
      "dump 123\n",
      "dump 124\n",
      "dump 125\n",
      "dump 126\n",
      "dump 127\n",
      "dump 128\n",
      "dump 129\n",
      "dump 130\n",
      "dump 131\n",
      "dump 132\n",
      "dump 133\n",
      "dump 134\n",
      "dump 135\n",
      "dump 136\n",
      "dump 137\n",
      "documents processed 3899276\n",
      "enwiki-20200801-pages-articles-multistream19.xml-p17620543p18754735.bz2\n",
      "dump 138\n",
      "dump 139\n",
      "dump 140\n",
      "dump 141\n",
      "dump 142\n",
      "dump 143\n",
      "dump 144\n",
      "dump 145\n",
      "dump 146\n",
      "dump 147\n",
      "dump 148\n",
      "documents processed 4216873\n",
      "enwiki-20200801-pages-articles-multistream2.xml-p30304p88444.bz2\n",
      "dump 149\n",
      "dump 150\n",
      "documents processed 4255652\n",
      "enwiki-20200801-pages-articles-multistream20.xml-p18754736p20254735.bz2\n",
      "dump 151\n",
      "dump 152\n",
      "dump 153\n",
      "dump 154\n",
      "dump 155\n",
      "dump 156\n",
      "dump 157\n",
      "dump 158\n",
      "dump 159\n",
      "dump 160\n",
      "dump 161\n",
      "dump 162\n",
      "dump 163\n",
      "dump 164\n",
      "dump 165\n",
      "dump 166\n",
      "documents processed 4706351\n",
      "enwiki-20200801-pages-articles-multistream20.xml-p20254736p21222156.bz2\n",
      "dump 167\n",
      "dump 168\n",
      "dump 169\n",
      "dump 170\n",
      "dump 171\n",
      "dump 172\n",
      "dump 173\n",
      "dump 174\n",
      "dump 175\n",
      "documents processed 4962600\n",
      "enwiki-20200801-pages-articles-multistream21.xml-p21222157p22722156.bz2.part\n",
      "dump 176\n",
      "dump 177\n",
      "dump 178\n",
      "dump 179\n",
      "dump 180\n",
      "dump 181\n",
      "dump 182\n",
      "dump 183\n",
      "documents processed 5200200\n",
      "enwiki-20200801-pages-articles-multistream21.xml-p22722157p23927983.bz2\n",
      "dump 184\n",
      "dump 185\n",
      "dump 186\n",
      "dump 187\n",
      "dump 188\n",
      "dump 189\n",
      "dump 190\n",
      "dump 191\n",
      "dump 192\n",
      "dump 193\n",
      "dump 194\n",
      "dump 195\n",
      "dump 196\n",
      "dump 197\n",
      "documents processed 5607529\n",
      "enwiki-20200801-pages-articles-multistream22.xml-p23927984p25427983.bz2\n",
      "dump 198\n",
      "dump 199\n",
      "dump 200\n",
      "dump 201\n",
      "dump 202\n",
      "dump 203\n",
      "dump 204\n",
      "dump 205\n",
      "dump 206\n",
      "dump 207\n",
      "dump 208\n",
      "dump 209\n",
      "dump 210\n",
      "dump 211\n",
      "dump 212\n",
      "documents processed 6055339\n",
      "enwiki-20200801-pages-articles-multistream22.xml-p25427984p26823660.bz2\n",
      "dump 213\n",
      "dump 214\n",
      "dump 215\n",
      "dump 216\n",
      "dump 217\n",
      "dump 218\n",
      "dump 219\n",
      "dump 220\n",
      "dump 221\n",
      "dump 222\n",
      "dump 223\n",
      "dump 224\n",
      "dump 225\n",
      "dump 226\n",
      "documents processed 6449219\n",
      "enwiki-20200801-pages-articles-multistream23.xml-p26823661p28323660.bz2\n",
      "dump 227\n",
      "dump 228\n",
      "dump 229\n",
      "dump 230\n",
      "dump 231\n",
      "dump 232\n",
      "dump 233\n",
      "dump 234\n",
      "dump 235\n",
      "dump 236\n",
      "dump 237\n",
      "dump 238\n",
      "dump 239\n",
      "dump 240\n",
      "dump 241\n",
      "dump 242\n",
      "documents processed 6927781\n",
      "enwiki-20200801-pages-articles-multistream23.xml-p28323661p29823660.bz2\n",
      "dump 243\n",
      "dump 244\n",
      "dump 245\n",
      "dump 246\n",
      "dump 247\n",
      "dump 248\n",
      "dump 249\n",
      "dump 250\n",
      "dump 251\n",
      "dump 252\n",
      "dump 253\n",
      "dump 254\n",
      "dump 255\n",
      "dump 256\n",
      "dump 257\n",
      "dump 258\n",
      "documents processed 7406148\n",
      "enwiki-20200801-pages-articles-multistream23.xml-p29823661p30503450.bz2\n",
      "dump 259\n",
      "dump 260\n",
      "dump 261\n",
      "dump 262\n",
      "dump 263\n",
      "dump 264\n",
      "dump 265\n",
      "dump 266\n",
      "documents processed 7617973\n",
      "enwiki-20200801-pages-articles-multistream3.xml-p88445p200509.bz2\n",
      "dump 267\n",
      "dump 268\n",
      "dump 269\n",
      "documents processed 7700263\n",
      "enwiki-20200801-pages-articles-multistream4.xml-p200510p352689.bz2\n",
      "dump 270\n",
      "dump 271\n",
      "dump 272\n",
      "documents processed 7784217\n",
      "enwiki-20200801-pages-articles-multistream5.xml-p352690p565313.bz2\n",
      "dump 273\n",
      "dump 274\n",
      "dump 275\n",
      "dump 276\n",
      "dump 277\n",
      "documents processed 7905083\n",
      "enwiki-20200801-pages-articles-multistream6.xml-p565314p892912.bz2\n",
      "dump 278\n",
      "dump 279\n",
      "dump 280\n",
      "dump 281\n",
      "dump 282\n",
      "dump 283\n",
      "dump 284\n",
      "documents processed 8095641\n",
      "enwiki-20200801-pages-articles-multistream7.xml-p892913p1268691.bz2\n",
      "dump 285\n",
      "dump 286\n",
      "dump 287\n",
      "dump 288\n",
      "dump 289\n",
      "dump 290\n",
      "dump 291\n",
      "documents processed 8291123\n",
      "enwiki-20200801-pages-articles-multistream8.xml-p1268692p1791079.bz2\n",
      "dump 292\n",
      "dump 293\n",
      "dump 294\n",
      "dump 295\n",
      "dump 296\n",
      "dump 297\n",
      "dump 298\n",
      "dump 299\n",
      "dump 300\n",
      "documents processed 8539659\n",
      "enwiki-20200801-pages-articles-multistream9.xml-p1791080p2336422.bz2\n",
      "dump 301\n",
      "dump 302\n",
      "dump 303\n",
      "dump 304\n",
      "dump 305\n",
      "dump 306\n",
      "dump 307\n",
      "dump 308\n",
      "dump 309\n",
      "documents processed 8796395\n",
      "final dump\n",
      "310  index_count\n",
      "\n",
      "token count 69513534\n",
      "time taken: 313.88320112469995\n",
      "total docs 8796395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import index_creation\n",
    "import xml.sax\n",
    "import timeit\n",
    "import subprocess\n",
    "import mwparserfromhell\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import Stemmer\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    start = timeit.default_timer()\n",
    "    global doc_ct,index_ct\n",
    "    \n",
    "#     data_path=sys.argv[1]\n",
    "#     data_path=\"wiki_dump/enwiki-20200801-pages-articles-multistream1.xml-p1p30303.bz2\"\n",
    "    data_directory=\"wiki_dump/\"\n",
    "    data_paths = [f for f in listdir(data_directory) if isfile(join(data_directory, f))]\n",
    "    data_paths.sort()\n",
    "    \n",
    "    \n",
    "    #data_paths is a list of names of all 34 wikidump file\n",
    "    \n",
    "\n",
    "    for data_path in data_paths:\n",
    "\n",
    "        print(data_path)\n",
    "\n",
    "        handler = WikiXmlHandler()\n",
    "        parser = xml.sax.make_parser()\n",
    "        parser.setContentHandler(handler)\n",
    "        doc_ct=0\n",
    "        for line in subprocess.Popen(['bzcat'], \n",
    "                                  stdin = open(data_directory+data_path), \n",
    "                                  stdout = subprocess.PIPE).stdout:\n",
    "            parser.feed(line)\n",
    "            \n",
    "        \n",
    "#             if len(handler._pages) > 1:\n",
    "#                 break\n",
    "        if(len(invertedIndex)):\n",
    "            dumpindex() \n",
    "        print(\"documents processed\",doc_id)\n",
    "\n",
    "\n",
    "    print(\"final dump\")\n",
    "    if(len(invertedIndex)):\n",
    "        dumpindex()\n",
    "    stat_path=\"stat.txt\"\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    stat_fp=open(stat_path,'w')\n",
    "    \n",
    "    token_count=merge(\"inverted_index\",index_ct)   \n",
    "\n",
    "    \n",
    "    print(\"token count\",token_count)\n",
    "    print(\"time taken:\",(stop-start)/60)\n",
    "    print(\"total docs\",doc_id)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\tmain() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
