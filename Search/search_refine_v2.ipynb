{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, b:Taj mahal, i:1999\n",
      "\n",
      "([4271407, 4504998, 1905557, 1573248, 7865321, 1789027, 8449929, 8187833, 7810043, 8729462], ['Manoj Bharathiraja', 'Tourism in Uttar Pradesh', 'List of sultans of the Maldives', 'Mohammed Uthman al-Mirghani al-Khatim', 'Gateway of India', 'Eastern journey of Nicholas II', 'Mumbai culture', 'List of state leaders in 1415', 'Gujranwala', 'Duke Erikson']) ##########################################################3\n",
      "10, t:russia relation\n",
      "\n",
      "([7180136, 5576419, 4796581, 6433295, 6433293, 6433292, 6433291, 4787904, 4488269, 4332535], ['US - Russia Relations', 'NZ-Russia Relations', 'Russia-NATO relations', 'UK - Russia relations', 'UK-Russia relations', 'Russia - UK relations', 'Russia-UK relations', 'NATO-Russia relations', 'DPRK-Russia relations', 'Russia-U.S. relations']) ##########################################################3\n",
      "10, i:Andrey Savin, b:Speedway World Cup\n",
      "\n",
      "([2144069, 701911, 6783434, 5509479, 7160587, 7492305, 3792764, 7070899, 5554788, 4365229], ['List of directors associated with art film', 'Wikipedia:Templates with red links/Athletics', '2010–11 Coppa Italia', 'Wikipedia:Templates with red links/029', '2019 in sports', '2018 in sports', 'Wikipedia:WikiProject Deletion sorting/Living people/archive', 'Saša Savič', 'Andreý Molçanow', 'Andrey Krylov (gymnast)']) ##########################################################3\n",
      "20, b:Mask of light, i:2003\n",
      "\n",
      "([6548973, 802503, 564211, 457931, 6109323, 8348428, 3918748, 3687384, 3687365, 3671792, 3671141, 3671078, 3192132, 461987, 8593303, 6651365, 5779071, 4231689, 3689586, 1581072], ['CMLL Super Viernes (March 2010)', 'Wikipedia:Reference desk/Archives/Science/March 2006', 'Wikipedia:Reference desk/Archives/Miscellaneous/January 2006', 'Wikipedia:Reference desk/Archives/Science/December 2005', 'Ligh microscope', 'Masked', 'Mask (comics)', 'Masks in theater and ritual', 'Masks in ritual and theatre', 'Masks in ritual', 'Masks and theatre', 'Masks in theatre', 'Lt. Kellaway', 'Stanley Ipkiss', 'Simultaneous masking', 'Tribal mask', 'Theatre mask', 'Temporal masking', 'Ritual mask', 'Black Mask 2']) ##########################################################3\n",
      "10, b:Redmond, i:Los Angeles Kings, c:1965 births\n",
      "\n",
      "([4157242, 1937379, 8561278, 8262161, 7946240, 5356165, 4411238, 4411235, 4411232, 7891009], ['Redmon and Vale', 'RedMon (Software)', 'Jessie Redmon Fausset', 'Jessie Fauset', 'Jessie Fausset', 'Redmon (disambiguation)', 'Redmon Bridge', 'Selah Creek Bridge', 'Fred Redmon Bridge', 'Redmon']) ##########################################################3\n",
      "10, Nolan Sisters 1979\n",
      "\n",
      "([2111622, 8272003, 633785, 6364037, 84713, 8663644, 8437904, 425798, 4612718, 7637052], ['Nolan Sisters (album)', 'The Nolans', 'Deanna Nolan', 'Linda Nolan', 'Bernie Nolan', 'Coleen Nolan', 'Frederick Nolan', 'Jeanette Nolan', 'Making Waves (The Nolans album)', 'Hannah and Her Sisters']) ##########################################################3\n",
      "5, Catcher in the Rye\n",
      "\n",
      "([7691672, 5287574, 4012186, 1918607, 8093379], ['Catcher in the Rye', 'Catcher In the Rye', 'A Catcher in the Rye', 'Catcher in the Rye (band)', 'Catcher in the rye']) ##########################################################3\n",
      "10, police chief association\n",
      "\n",
      "([8011976, 7982107, 5254312, 7942774, 7890578, 2675265, 7967801, 2675266, 7876082, 713197], ['Chief of police', 'Chief constable', 'Chief ambulance officer', 'Police authority', 'Police dog', 'Association of chief police officers', 'Chief', 'Association of chief police officers in scotland', 'Police corruption', 'Police misconduct']) ##########################################################3\n",
      "10, b:1898, c:Leicester City Football League\n",
      "\n",
      "([8024190, 6834074, 6833893, 6702063, 6536896, 6492767, 6306385, 6293732, 6293602, 6293520], ['189 BCE', 'Category:1894 crimes', 'Category:1892 crimes', 'Category:1896 crimes', 'Category:1890 crimes', 'Category:1895 crimes', 'Category:1897 crimes', 'Category:1893 crimes', 'Category:1899 crimes', 'Category:1891 crimes']) ##########################################################3\n",
      "10, Chongqing University\n",
      "\n",
      "([5407767, 5407772, 5407771, 5407769, 7682839, 7879823, 4844929, 8110370, 8110366, 6486987], ['Chongqing institute of technology', 'Chongqing university of science and technology', 'Chongqing university of posts and telecommunications', 'Chongqing technology and business university', 'Chongqing', 'Chongqing University', 'List of universities and colleges in Chongqing', 'Chongqing Jiaotong University', 'Chongqing Normal University', 'Chongqing University of Technology']) ##########################################################3\n",
      "20, t:Virgin Islands, r:geonames, c:islands\n",
      "\n",
      "([1834838, 6498953, 8263014, 8263007, 8111080, 8007260, 7851102, 7673314, 7629221, 7622121, 7622120, 7210267, 7014800, 6978754, 6900520, 6874415, 6286125, 6082697, 5709728, 5511651], ['Virgin islands', 'Virgin islands job market', 'US Virgin Islands national soccer team', 'U.S. Virgin Islands national football team', 'Virgin Islander music', 'U. S. Virgin Islands', 'Virgin Islands of the U.S.', 'US Virgin Islands', 'Virgin Islands, U.S.', 'Virgin Islands (US)', 'Virgin Islands (UK)', 'U.S. Virgin Islands DHS', 'U.S. Virgin Islander', 'Virgin Islands worm lizard', 'U. S. Virgin Islands national soccer team', 'U.S. Virgin Islands national baseball team', 'Virgin Islands dwarf sphaero', 'Virgin Islands (disambiguation)', 'UK Virgin Islands records in athletics', 'TV2 (Virgin Islands TV channel)']) ##########################################################3\n",
      "20, b:Dennis Lewiston, c:1934 births\n",
      "\n",
      "([8778949, 8715191, 8577776, 8577775, 8497886, 7971712, 7971711, 7971710, 7971708, 6449450, 6365140, 6281170, 6281168, 6281166, 5008783, 5006725, 4932140, 4404656, 4216424, 4083026], ['C.J. Dennis', 'Dennis Wyatt Chavez', 'Denis Martinez', 'Denis Martínez', 'Thomas Anthony Denny', \"Denny's Diners\", \"Denny's Restaurants\", \"Denny's Restaurant\", \"Denny's Diner\", 'Tyler Dennis', 'Dennys Quiñonez', 'Slim Slam', 'All-American Slam', 'Lumberjack Slam', 'Denny Andreina Méndez de la Rosa', 'Nanerpus', 'Lumberjack slam', 'Denny Martinez', 'DFO, LLC', 'DENN']) ##########################################################3\n",
      "20, boeing 341\n",
      "\n",
      "([3176788, 7696399, 2958, 310823, 6621790, 3672459, 7669939, 2514414, 7749024, 1495561, 8511482, 7737322, 1696447, 622731, 8398008, 1964756, 7716788, 7821967, 8208673, 5265352], ['Template:Boeing model numbers', 'Boeing B-47 Stratojet', 'Boeing B-17 Flying Fortress', 'Wien Air Alaska', '781st Bombardment Squadron', '147th Air Refueling Squadron', 'Wright-Patterson Air Force Base', 'Grayson High School', 'Birmingham Airport', 'Skiathos International Airport', 'Graz Airport', 'Memphis International Airport', 'Air raids on Japan', 'Auckland Zoo', 'Cabin pressurization', '341st Missile Wing', 'Maersk Air', 'British Caledonian', '1984 United States Senate elections', 'List of accidents and incidents involving airliners in the United States']) ##########################################################3\n",
      "5, b:Terence, i:Fields Medal\n",
      "\n",
      "([7786260, 6951676, 3465462, 8662318, 8610496], ['Publius Terentius Afer', 'Terrence', 'Homo sum', 'Terence Powderly', 'Terrence McKenna']) ##########################################################3\n",
      "10, b:Big Four, i:Agatha Christie\n",
      "\n",
      "([3930762, 8784183, 1034961, 220985, 5126911, 706908, 780505, 6944187, 5553781, 4699843], ['Fou (instrument)', 'Mad Love', 'Danielle Proulx', 'Lionel White', 'Viking Formation', 'Template:The Sopranos', 'Club X', 'Angie Be', \"List of Disney's Beauty and the Beast characters\", 'Le Combat Continue']) ##########################################################3\n",
      "10, b:Speed of light, c:Concepts in physics\n",
      "\n",
      "([4802389, 7406668, 8190838, 6320017, 4083789, 2948160, 457931, 8183601, 802503, 674995], ['Wikipedia:Reference desk/Archives/Science/2008 December 9', 'Wikipedia:Reference desk/Archives/Science/2010 November 27', 'Colitis', 'Wikipedia:Reference desk/Archives/Miscellaneous/2010 February 22', 'Wikipedia:Coverage of Mathworld topics/B', 'Wikipedia:Reference desk/Archives/Science/2007 November 30', 'Wikipedia:Reference desk/Archives/Science/December 2005', 'Wikipedia:Reference desk/Archives/October 2004 I', 'Wikipedia:Reference desk/Archives/Science/March 2006', 'List of Statutory Instruments of the United Kingdom, 1997']) ##########################################################3\n",
      "20, Blade Runner\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([8650993, 8612804, 7986477, 7986441, 7843327, 7355697, 5011555, 4346777, 4336157, 4336086, 3205982, 3059439, 1616272, 1097979, 775230, 617913, 2148, 8091517, 480165, 8621759], ['Blade Runner (themes)', 'Blade Runner (soundtrack)', 'Blade Runner(computer game)', 'Blade Runner (computer game)', 'Blade Runner (1997 video game)', 'Blade Runner soundtrack', 'Blade Runner (soundtracks)', 'Blade Runner (1985 videogame)', 'Blade Runner (1985 video game)', 'Blade Runner (video game)', 'Blade Runner soundtracks', 'Blade Runner (versions)', 'Blade Runner (game)', 'Blade Runner (disambiguation)', 'Blade Runner (videogame)', 'Blade Runner (film)', 'Blade Runner', 'Blade runner', 'T-Blades', 'Themes in Blade Runner']) ##########################################################3\n",
      "5, v for vendetta\n",
      "\n",
      "([5036069, 4035925, 3608703, 1438542, 8587664], ['File:Portman V.jpg', 'File:Norsefire-logo.png', 'File:250px-Sutler2.jpg', 'File:Adam Susan.jpg', 'V for Vendetta (film)']) ##########################################################3\n",
      "3, b:V for Vendetta, i:2006\n",
      "\n",
      "([6291082, 5252940, 5027212], ['FC Forli', 'Go For It!', 'For very large values of 2']) ##########################################################3\n",
      "5, young's modulus\n",
      "\n",
      "([13207, 8422885, 7546961, 1843775, 771222], ['Modulus', 'Young Modulus', \"Youngs' Modulus\", 'Youngs Modulus', \"Young's Modulus\"]) ##########################################################3\n",
      "10, b:Speed of light, c:Concepts in physics\n",
      "\n",
      "([4802389, 7406668, 8190838, 6320017, 4083789, 2948160, 457931, 8183601, 802503, 674995], ['Wikipedia:Reference desk/Archives/Science/2008 December 9', 'Wikipedia:Reference desk/Archives/Science/2010 November 27', 'Colitis', 'Wikipedia:Reference desk/Archives/Miscellaneous/2010 February 22', 'Wikipedia:Coverage of Mathworld topics/B', 'Wikipedia:Reference desk/Archives/Science/2007 November 30', 'Wikipedia:Reference desk/Archives/Science/December 2005', 'Wikipedia:Reference desk/Archives/October 2004 I', 'Wikipedia:Reference desk/Archives/Science/March 2006', 'List of Statutory Instruments of the United Kingdom, 1997']) ##########################################################3\n",
      "20, b:angus, r:tories, i:James Lee\n",
      "\n",
      "([797941, 3645823, 3601875, 3459508, 7954877, 2284701, 3035953, 3036422, 8352409, 5840647, 2278103, 519729, 2546368, 3336277, 8151639, 3156789, 2828993, 3716675, 2299914, 2260838], ['Kukukuku', 'Angues', 'Angues, Huesca', 'Angues, Spain', 'Maria Nsue Angue', 'Poroto Angu', 'File:Angu-Mashoko-Front-Cover.jpg', 'Angu Mashoko', 'Volta Grande', 'Purificación', 'Mikania millei', 'Angu', 'Angüés', 'MVM Arts and Science College', 'Narrow leaf-toed gecko', 'Ji Xu', 'Ankalamma', 'Anguía District', 'Roskilde Festival 2005', 'Anangu Pitjantjatjara Yankunytjatjara Land Rights Act 1981']) ##########################################################3\n",
      "30, b:Taxicab 1729\n",
      "\n",
      "([7966702, 289006, 7875872, 7876197, 1002578, 7872385, 7904928, 1351404, 6005, 7781113, 7705466, 7914673, 7836169, 7826223, 7844043, 4228855, 8719903, 7777820, 7619985, 4232828, 7788623, 4232761, 4227483, 452785, 8029360, 7541583, 5287785, 4151885, 3952273, 2486687], ['Generalized taxicab number', '4104 (number)', '1729 (number)', 'Taxicab number', \"Wikipedia:Evaluating how interesting an integer's mathematical property is\", 'Interesting number paradox', 'Bernard Frénicle de Bessy', 'Injective metric space', \"Euler's sum of powers conjecture\", 'List of number theory topics', 'List of numbers', 'Joseph Force Crater', '1000 (number)', 'Wikipedia:WikiProject Numbers', 'Kandahar Province', 'Srinivasa Ramanujan', 'Toyota Highlander', 'Bender (Futurama)', 'Winchester, Virginia', 'Hippocampus', 'History of Paris', 'New Orleans', 'Manhattan', 'Timeline of Paris', 'Taxi cab', 'Tacksey', 'Taxidriver', 'Cab Driver', 'Taxi (transport)', 'Cabdriver']) ##########################################################3\n",
      "50, b:Marc Spector, i:Marvel Comics, c:1980 comics debuts\n",
      "\n",
      "([4235461, 2557019, 2538681, 8368090, 8302347, 8096543, 8007293, 7244749, 5605116, 5345482, 5333637, 5201902, 4321360, 4034352, 3472184, 2991212, 2962777, 2679037, 2077888, 1308056, 20737, 6740853, 6740851, 8782186, 8766478, 8766477, 8742957, 8727697, 8703129, 8674347, 8658573, 8596431, 8587098, 8543761, 8519847, 8489792, 8391294, 8362808, 8330018, 8251260, 8190193, 8190192, 8190191, 8133510, 8114983, 8093971, 8073303, 8009772, 8004280, 7997320], ['Situationist International', 'Wikipedia:WikiProject Deletion sorting/Visual arts/archive', 'Wikipedia:WikiProject Deletion sorting/United States of America/archive', 'Gamil Gharbi', 'E Quatremère', 'Marc Forné', 'MARC 21', 'Marc Bélanger (journalist)', \"Bernie Shulman's\", 'Marc Gené i Guerrero', 'Marc Bertran', 'Caussidière', 'Marc Torrejón Moya', 'Marc Bruère Desrivaux', 'Mark O Se', 'GE Mark V', 'Marc elpine', 'Marc Kuehne', 'E. Quatremère', 'Gamil Rodrigue Gharbi', 'Mark Lepine', 'CANMARC', 'CAN/MARC', 'Mark Emery', 'Maryland Area Regional Commuter', 'Maryland Rail Commuter', 'Marc Birgikt', 'Marcus Iavaroni', 'Mark Wallice', 'Marc Théodore Bourrit', 'Marc Alexander (Honolulu priest)', 'Marc W. Kirschner', 'Mark Klaw', 'Bourrit', 'Johnny B. Badd', 'Marc Ouelette', 'Saint-Marc Girardin', 'Vice Admiral Mitscher', 'Mark Laidlaw', 'Lescarbot', 'Mark Dutrou', 'Mark Dutroux', 'Marc Dutrou', 'MARC (rail)', 'Mark Racicot', 'T A G', 'Mark Okrand', 'M. W. Buie', 'Mark Grossman', 'Marc Feld']) ##########################################################3\n",
      "100, Sudebnik\n",
      "\n",
      "([8118701, 7667975, 6990120, 5008993, 657602, 686109, 2969749, 856508, 8002031, 4223166, 8404030, 8155088, 7884799, 7224112, 7755407, 5968867, 1253570, 7127486, 4244343, 1693374, 8303250, 7626963, 16639, 8979], ['Sudebnik of 1497', 'Ivan III of Russia', 'Sudebnik of Kazimir', 'Forbidden years', 'Sobornoye Ulozheniye', 'Pskov Judicial Charter', 'Novgorod Judicial Charter', \"George's Day in Autumn\", 'Russkaya Pravda', '1497', 'Serfdom in Russia', 'Vasily Tatishchev', 'Law of Russia', 'List of Russian historians', 'Pskov', 'William E. Butler', 'Capital punishment in Russia', 'List of Russian scientists', 'Ivan the Terrible', 'Wikipedia:WikiProject Russia/New article announcements/January 2007', 'Early modern period', 'List of Russian people', 'Russia', 'History of Russia']) ##########################################################3\n",
      "End of processing\n"
     ]
    }
   ],
   "source": [
    "# import Stemmer\n",
    "import re\n",
    "import bisect\n",
    "import xml.sax\n",
    "import timeit\n",
    "import subprocess\n",
    "import mwparserfromhell\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import Stemmer\n",
    "import re\n",
    "import math\n",
    "import timeit\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "reg1 = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',re.DOTALL)\n",
    "reg2 = re.compile(r'{\\|(.*?)\\|}',re.DOTALL)\n",
    "reg3 = re.compile(r'{{v?cite(.*?)}}',re.DOTALL)\n",
    "refreg=r'{{v?cite(.*?)}}'\n",
    "reg4 = re.compile(r'[-.,:;_?()\"/\\']',re.DOTALL)\n",
    "reg5 = re.compile(r'\\[\\[file:(.*?)\\]\\]',re.DOTALL)\n",
    "# reg6 = re.compile(r'[\\'~` \\n\\\"_!=@#$%-^*+{\\[}\\]\\|\\\\<>/?]',re.DOTALL)\n",
    "reg6 = re.compile(r'[\\'~`\\\"_!=@#$%\\-^*+{\\[}\\]\\|\\\\<>/?]',re.DOTALL)\n",
    "\n",
    "catRegExp = r'\\[\\[category:(.*?)\\]\\]'\n",
    "infoRegExp = r'{{infobox(.*?)}}'\n",
    "refRegExp = r'== ?references ?==(.*?)=='\n",
    "reg7 = re.compile(infoRegExp,re.DOTALL)\n",
    "reg8 = re.compile(refRegExp,re.DOTALL)\n",
    "reg9 = re.compile(r'{{(.*?)}}',re.DOTALL)\n",
    "reg10 = re.compile(r'<(.*?)>',re.DOTALL)\n",
    "reg11=re.compile(r'\\[\\[(.*?)\\]\\]')\n",
    "reg12=re.compile(catRegExp,re.DOTALL)\n",
    "\n",
    "index_path=\"inverted_index/merged_path/\"\n",
    "\n",
    "def gettitles(toplist):\n",
    "    top_titles=[]\n",
    "    for docid in toplist:\n",
    "        title_file_no=math.floor(docid/30000)\n",
    "        title_index=docid%30000\n",
    "        fp=open(\"title/\"+str(title_file_no),'r')\n",
    "        \n",
    "        i=0\n",
    "        for line in fp.readlines():\n",
    "            if(i==title_index):\n",
    "                top_titles.append(line[:-1])\n",
    "            i+=1\n",
    "    return top_titles\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    " \n",
    "def convert(titledict):\n",
    "    resdict={}\n",
    "    for key,val in titledict.items():\n",
    "        val=val.strip()\n",
    "        token=val.split()\n",
    "        token=stemmer.stemWords(token)\n",
    "        for t in token:\n",
    "           \n",
    "            resdict[t]=key\n",
    "    return resdict\n",
    "\n",
    "def getposting(index_path,word):\n",
    "#     print(word)\n",
    "    secondaryindex_fp=open(index_path+\"secondary_index.txt\",'r')\n",
    "    secondary_list=[]\n",
    "    for line in secondaryindex_fp.readlines():\n",
    "        secondary_list.append(line[:-1])\n",
    "        \n",
    "    index_no=bisect.bisect(secondary_list, word)-1 #index number fetched from secondary index\n",
    "    if index_no==-1:                  #invalid index number\n",
    "        return \"\"\n",
    "    index_path=index_path+str(index_no)+\".txt\"   # path to index file to be searched\n",
    "    index_fp=open(index_path,'r')\n",
    "    \n",
    "    for entry in index_fp.readlines():\n",
    "        idx=entry.index('#')\n",
    "        w=entry[:idx]\n",
    "        if(w==word):\n",
    "            return entry[idx:]\n",
    "    return \"\"                                   # if posting doesn't exist return empty string\n",
    "\n",
    "def gettopten(docid_freq_tfidf,top_k):\n",
    "    rank_list=[]\n",
    "    top_ten=[]\n",
    "    \n",
    "    for docid in docid_freq_tfidf:\n",
    "#         print(docid)\n",
    "        rank_list.append([[docid_freq_tfidf[docid][0],docid_freq_tfidf[docid][1]],docid])\n",
    "        \n",
    "    rank_list.sort(reverse=True)              # if number of query tokens are same in doc1 and doc2 then go for tfidf           \n",
    "    \n",
    "    i=0\n",
    "    for item in rank_list:\n",
    "        top_ten.append(item[1])\n",
    "        i+=1\n",
    "        if i == top_k:\n",
    "            break\n",
    "            \n",
    "    return top_ten\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "# index_path=\"inverted_index/merged_path/\"\n",
    "\n",
    "# index_path=\"inverted_index/merged_path/\"\n",
    "\n",
    "\n",
    "\n",
    "def top_search1(st,top_k):\n",
    "\n",
    "    st=st.lower()\n",
    "    index=st.find(':')\n",
    "    docid_freq_tfidf={}  # dict keys are doc_id and value is list of frequescy and tfidf weight\n",
    "                         # where frequency is no of tokens from query are present in doc \n",
    "                         # and tfidf weight is the sum of all tfidfs of word\n",
    "    if(index>=0):\n",
    "        titledict={}\n",
    "\n",
    "        field='#' \n",
    "        buffer=\"\"\n",
    "        for i in range(len(st)):\n",
    "            if st[i]==':':\n",
    "                if field!='#':\n",
    "                    titledict[field]=buffer[0:len(buffer)-1]\n",
    "                    buffer=\"\"\n",
    "                field=st[i-1]\n",
    "                buffer=\"\"\n",
    "\n",
    "            else:\n",
    "                buffer=buffer+st[i]\n",
    "\n",
    "        if field!='#':\n",
    "            titledict[field]=buffer\n",
    "        resdict=convert(titledict)     #dict of word and their field type\n",
    "#         print(resdict)\n",
    "\n",
    "\n",
    "        for word,field in resdict.items():\n",
    "#             print(word,field,\"#####################################################################################################################################################################################################################################################3\")\n",
    "            postings=getposting(index_path,word)\n",
    "    #         print(postings)\n",
    "\n",
    "            if postings==\"\":\n",
    "                continue\n",
    "\n",
    "\n",
    "            postings=postings[:-1]  #remove new line\n",
    "\n",
    "            postings=postings.split('#')\n",
    "            postings=postings[1:]   #first ele in postings is always \"\" so remove it\n",
    "    #         print(postings)\n",
    "\n",
    "            for posting in postings:\n",
    "            \n",
    "            \n",
    "                found=False\n",
    "                if field=='c':\n",
    "                    postinglist=re.findall(r'c[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                        \n",
    "                if field=='i':\n",
    "                    postinglist=re.findall(r'i[0-9]+',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                    \n",
    "                if field=='t':\n",
    "                    postinglist=re.findall(r't[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if field=='e':\n",
    "                    postinglist=re.findall(r'e[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if field=='b':\n",
    "                    postinglist=re.findall(r'b[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if field=='r':\n",
    "                    postinglist=re.findall(r'r[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if(found==False):     #field not found so overall tfidf returned\n",
    "                    poslist=posting.split('@')\n",
    "                    tfidf=int(poslist[1])\n",
    "                    prepost=poslist[0]\n",
    "\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "\n",
    "                    if docid not in docid_freq_tfidf:\n",
    "                        fre_tfidfsum=[1,tfidf]\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                    else:\n",
    "                        fre_tfidfsum=docid_freq_tfidf[docid]\n",
    "                        fre_tfidfsum[0]+=1\n",
    "                        fre_tfidfsum[1]+=tfidf\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                else:   #field found so field's tfidf considered\n",
    "#                     poslist=posting.split('@')\n",
    "#                     tfidf=int(poslist[1])\n",
    "#                     prepost=poslist[0]\n",
    "\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "\n",
    "                    if docid not in docid_freq_tfidf:\n",
    "                        fre_tfidfsum=[1,tfidf]\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                    else:\n",
    "                        fre_tfidfsum=docid_freq_tfidf[docid]\n",
    "                        fre_tfidfsum[0]+=1\n",
    "                        fre_tfidfsum[1]+=tfidf\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        top_ten=gettopten(docid_freq_tfidf,top_k) #now working for to k instead of top 10.\n",
    "        top_titles=gettitles(top_ten)\n",
    "#         print(top_ten)\n",
    "#         print(top_titles)\n",
    "\n",
    "    else:\n",
    "        st=reg1.sub(' ',st)\n",
    "        st=reg2.sub(' ',st)\n",
    "        st=reg3.sub(' ',st)\n",
    "        st=reg4.sub(' ',st)\n",
    "        st=reg5.sub(' ',st)\n",
    "        st=reg6.sub(' ',st)\n",
    "        st=st.strip()\n",
    "        st=st.split()\n",
    "\n",
    "        st=[word for word in st if word not in stop_words]\n",
    "        st=stemmer.stemWords(st)\n",
    "\n",
    "        for word in st:\n",
    "#             print(word)\n",
    "            postings=getposting(index_path,word)\n",
    "    #         if word.isnumeric():\n",
    "    #             print(postings)\n",
    "            if postings==\"\":\n",
    "                continue\n",
    "\n",
    "\n",
    "            postings=postings[:-1]  #remove new line\n",
    "\n",
    "            postings=postings.split('#')\n",
    "            postings=postings[1:] \n",
    "\n",
    "            for posting in postings:\n",
    "\n",
    "\n",
    "\n",
    "#                 poslist=posting.split('@')\n",
    "#                 tfidf=int(poslist[1])-10\n",
    "#                 prepost=poslist[0]\n",
    "                \n",
    "                #precedence order  title>infobox=category>ref=external link>body\n",
    "                if(len(re.findall(r't[0-9]*',posting,re.DOTALL))):\n",
    "                    postinglist=re.findall(r't[0-9]*',posting,re.DOTALL)\n",
    "                    tfidf=tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "                elif(len(re.findall(r'i[0-9]+',posting,re.DOTALL))):\n",
    "                    postinglist=re.findall(r'i[0-9]+',posting,re.DOTALL)\n",
    "                    tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "                elif(len(re.findall(r'c[0-9]*',posting,re.DOTALL))):\n",
    "                    postinglist=re.findall(r'c[0-9]*',posting,re.DOTALL)\n",
    "                    tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "                elif(len(re.findall(r'r[0-9]*',posting,re.DOTALL))):\n",
    "                    postinglist=re.findall(r'r[0-9]*',posting,re.DOTALL)\n",
    "                    tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "                elif(len(re.findall(r'e[0-9]*',posting,re.DOTALL))):\n",
    "                    postinglist=re.findall(r'e[0-9]*',posting,re.DOTALL)\n",
    "                    tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "                elif(len(re.findall(r'b[0-9]*',posting,re.DOTALL))):\n",
    "                    postinglist=re.findall(r'b[0-9]*',posting,re.DOTALL)\n",
    "                    tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "                \n",
    "\n",
    "\n",
    "#                 docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "#                 docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "\n",
    "                if docid not in docid_freq_tfidf:\n",
    "                    fre_tfidfsum=[1,tfidf]\n",
    "                    docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                else:\n",
    "                    fre_tfidfsum=docid_freq_tfidf[docid]\n",
    "                    fre_tfidfsum[0]+=1\n",
    "                    fre_tfidfsum[1]+=tfidf\n",
    "                    docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "\n",
    "        top_ten=gettopten(docid_freq_tfidf,top_k)   #working for top k now\n",
    " \n",
    "        top_titles=gettitles(top_ten)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return (top_ten,top_titles)\n",
    "            \n",
    "            \n",
    "def main():\n",
    "\n",
    "    query_in=open(\"queries.txt\",'r')\n",
    "    query_out=open(\"queries_op.txt\",'w')\n",
    "\n",
    "    for line in query_in.readlines():\n",
    "        print(line)\n",
    "        line=line.split(',')\n",
    "        top_k=int(line[0])\n",
    "        query=line[1][:-1]\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        ids_titles=top_search1(query,top_k)         #main search logic return tuple of ids and titles list\n",
    "        print(ids_titles,\"##########################################################3\")\n",
    "        end = timeit.default_timer()\n",
    "\n",
    "        for n in range(top_k):\n",
    "            if n >= len(ids_titles[0]):       #if ids_titles doesn't have the number of documents we are asking for\n",
    "                break\n",
    "            write_line=str(ids_titles[0][n])+\",\"+str(ids_titles[1][n])+\"\\n\"\n",
    "            query_out.write(write_line)\n",
    "\n",
    "        t=end-start\n",
    "        query_out.write(str(round(t,3))+\",\"+str(round(t/top_k,3))+\"\\n\")\n",
    "        query_out.write(\"\\n\")\n",
    "\n",
    "    query_out.close()\n",
    "    print(\"End of processing\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\tmain() \n",
    "        \n",
    "        \n",
    "#     query_out.write(\"\\n\")\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "           \n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "# else:\n",
    "#     st=st.strip()\n",
    "#     tokens=st.split()\n",
    "#     tokens=stemmer.stemWords(tokens)\n",
    "#     fp=open('demo.txt','r')\n",
    "#     lines=fp.readlines()\n",
    "#     for line in lines:\n",
    "#         idx=line.find('#')\n",
    "#         word=line[0:idx]\n",
    "#         if word in tokens:\n",
    "#             print(line)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
