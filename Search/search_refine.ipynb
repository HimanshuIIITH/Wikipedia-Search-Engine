{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7, Manmohan singh\n",
      "\n",
      "([1919962, 6883809, 3737046, 1273307, 1273340, 3229970, 6234406, 3067317, 5988706, 6602299], ['Dr Manmohan Singh', 'Manmohan singh', 'Dr Manmohan Singh Scholarships', 'Manmohan Singh (cinematographer)', 'Manmohan Singh, Director', 'Man Mohan Singh', 'First Manmohan Singh Cabinet', 'ਮਨਮੋਹਨ ਸਿੰਘ', 'Justice Liberhan', '2004 Council of Ministers of India']) ##########################################################3\n",
      "7, the taj\n",
      "\n",
      "([7785515, 4614205, 3116028, 7296087, 7104930, 6391257, 6307738, 6220978, 6220965, 4741049], ['Tajín', 'Taj+mahal', 'Taj mahel', 'Taj Mahaj', 'Taj-ul-Masjid', 'Taj Muhammad', 'Taj (disambiguation)', 'Taj ad-Din', 'Taj El-Din', 'Taj Hotel']) ##########################################################3\n",
      "End of processing\n"
     ]
    }
   ],
   "source": [
    "# import Stemmer\n",
    "import re\n",
    "import bisect\n",
    "import xml.sax\n",
    "import timeit\n",
    "import subprocess\n",
    "import mwparserfromhell\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import Stemmer\n",
    "import re\n",
    "import math\n",
    "import timeit\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "reg1 = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',re.DOTALL)\n",
    "reg2 = re.compile(r'{\\|(.*?)\\|}',re.DOTALL)\n",
    "reg3 = re.compile(r'{{v?cite(.*?)}}',re.DOTALL)\n",
    "refreg=r'{{v?cite(.*?)}}'\n",
    "reg4 = re.compile(r'[-.,:;_?()\"/\\']',re.DOTALL)\n",
    "reg5 = re.compile(r'\\[\\[file:(.*?)\\]\\]',re.DOTALL)\n",
    "# reg6 = re.compile(r'[\\'~` \\n\\\"_!=@#$%-^*+{\\[}\\]\\|\\\\<>/?]',re.DOTALL)\n",
    "reg6 = re.compile(r'[\\'~`\\\"_!=@#$%\\-^*+{\\[}\\]\\|\\\\<>/?]',re.DOTALL)\n",
    "\n",
    "catRegExp = r'\\[\\[category:(.*?)\\]\\]'\n",
    "infoRegExp = r'{{infobox(.*?)}}'\n",
    "refRegExp = r'== ?references ?==(.*?)=='\n",
    "reg7 = re.compile(infoRegExp,re.DOTALL)\n",
    "reg8 = re.compile(refRegExp,re.DOTALL)\n",
    "reg9 = re.compile(r'{{(.*?)}}',re.DOTALL)\n",
    "reg10 = re.compile(r'<(.*?)>',re.DOTALL)\n",
    "reg11=re.compile(r'\\[\\[(.*?)\\]\\]')\n",
    "reg12=re.compile(catRegExp,re.DOTALL)\n",
    "\n",
    "index_path=\"inverted_index/merged_path/\"\n",
    "\n",
    "def gettitles(toplist):\n",
    "    top_titles=[]\n",
    "    for docid in toplist:\n",
    "        title_file_no=math.floor(docid/30000)\n",
    "        title_index=docid%30000\n",
    "        fp=open(\"title/\"+str(title_file_no),'r')\n",
    "        \n",
    "        i=0\n",
    "        for line in fp.readlines():\n",
    "            if(i==title_index):\n",
    "                top_titles.append(line[:-1])\n",
    "            i+=1\n",
    "    return top_titles\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    " \n",
    "def convert(titledict):\n",
    "    resdict={}\n",
    "    for key,val in titledict.items():\n",
    "        val=val.strip()\n",
    "        token=val.split()\n",
    "        token=stemmer.stemWords(token)\n",
    "        for t in token:\n",
    "           \n",
    "            resdict[t]=key\n",
    "    return resdict\n",
    "\n",
    "def getposting(index_path,word):\n",
    "#     print(word)\n",
    "    secondaryindex_fp=open(index_path+\"secondary_index.txt\",'r')\n",
    "    secondary_list=[]\n",
    "    for line in secondaryindex_fp.readlines():\n",
    "        secondary_list.append(line[:-1])\n",
    "        \n",
    "    index_no=bisect.bisect(secondary_list, word)-1 #index number fetched from secondary index\n",
    "    if index_no==-1:                  #invalid index number\n",
    "        return \"\"\n",
    "    index_path=index_path+str(index_no)+\".txt\"   # path to index file to be searched\n",
    "    index_fp=open(index_path,'r')\n",
    "    \n",
    "    for entry in index_fp.readlines():\n",
    "        idx=entry.index('#')\n",
    "        w=entry[:idx]\n",
    "        if(w==word):\n",
    "            return entry[idx:]\n",
    "    return \"\"                                   # if posting doesn't exist return empty string\n",
    "\n",
    "def gettopten(docid_freq_tfidf):\n",
    "    rank_list=[]\n",
    "    top_ten=[]\n",
    "    \n",
    "    for docid in docid_freq_tfidf:\n",
    "#         print(docid)\n",
    "        rank_list.append([[docid_freq_tfidf[docid][0],docid_freq_tfidf[docid][1]],docid])\n",
    "        \n",
    "    rank_list.sort(reverse=True)              # if number of query tokens are same in doc1 and doc2 then go for tfidf           \n",
    "    \n",
    "    i=0\n",
    "    for item in rank_list:\n",
    "        top_ten.append(item[1])\n",
    "        i+=1\n",
    "        if i == 10:\n",
    "            break\n",
    "            \n",
    "    return top_ten\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "# index_path=\"inverted_index/merged_path/\"\n",
    "\n",
    "# index_path=\"inverted_index/merged_path/\"\n",
    "\n",
    "\n",
    "\n",
    "def top_search1(st):\n",
    "\n",
    "    st=st.lower()\n",
    "    index=st.find(':')\n",
    "    docid_freq_tfidf={}  # dict keys are doc_id and value is list of frequescy and tfidf weight\n",
    "                         # where frequency is no of tokens from query are present in doc \n",
    "                         # and tfidf weight is the sum of all tfidfs of word\n",
    "    if(index>=0):\n",
    "        titledict={}\n",
    "\n",
    "        field='#' \n",
    "        buffer=\"\"\n",
    "        for i in range(len(st)):\n",
    "            if st[i]==':':\n",
    "                if field!='#':\n",
    "                    titledict[field]=buffer[0:len(buffer)-1]\n",
    "                    buffer=\"\"\n",
    "                field=st[i-1]\n",
    "                buffer=\"\"\n",
    "\n",
    "            else:\n",
    "                buffer=buffer+st[i]\n",
    "\n",
    "        if field!='#':\n",
    "            titledict[field]=buffer\n",
    "        resdict=convert(titledict)     #dict of word and their field type\n",
    "#         print(resdict)\n",
    "\n",
    "\n",
    "        for word,field in resdict.items():\n",
    "#             print(word,field,\"#####################################################################################################################################################################################################################################################3\")\n",
    "            postings=getposting(index_path,word)\n",
    "    #         print(postings)\n",
    "\n",
    "            if postings==\"\":\n",
    "                continue\n",
    "\n",
    "\n",
    "            postings=postings[:-1]  #remove new line\n",
    "\n",
    "            postings=postings.split('#')\n",
    "            postings=postings[1:]   #first ele in postings is always \"\" so remove it\n",
    "    #         print(postings)\n",
    "\n",
    "            for posting in postings:\n",
    "            \n",
    "            \n",
    "                found=False\n",
    "                if field=='c':\n",
    "                    postinglist=re.findall(r'c[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                        \n",
    "                if field=='i':\n",
    "                    postinglist=re.findall(r'i[0-9]+',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                    \n",
    "                if field=='t':\n",
    "                    postinglist=re.findall(r't[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if field=='e':\n",
    "                    postinglist=re.findall(r'e[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if field=='b':\n",
    "                    postinglist=re.findall(r'b[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if field=='r':\n",
    "                    postinglist=re.findall(r'r[0-9]*',posting,re.DOTALL)\n",
    "                    if(len(postinglist)):\n",
    "                        tfidf=int(re.findall(r'[0-9]+',postinglist[0],re.DOTALL)[0])\n",
    "                        found=True\n",
    "                if(found==False):     #field not found so overall tfidf returned\n",
    "                    poslist=posting.split('@')\n",
    "                    tfidf=int(poslist[1])\n",
    "                    prepost=poslist[0]\n",
    "\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "\n",
    "                    if docid not in docid_freq_tfidf:\n",
    "                        fre_tfidfsum=[1,tfidf]\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                    else:\n",
    "                        fre_tfidfsum=docid_freq_tfidf[docid]\n",
    "                        fre_tfidfsum[0]+=1\n",
    "                        fre_tfidfsum[1]+=tfidf\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                else:   #field found so field's tfidf considered\n",
    "#                     poslist=posting.split('@')\n",
    "#                     tfidf=int(poslist[1])\n",
    "#                     prepost=poslist[0]\n",
    "\n",
    "                    docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "\n",
    "                    docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "\n",
    "                    if docid not in docid_freq_tfidf:\n",
    "                        fre_tfidfsum=[1,tfidf]\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                    else:\n",
    "                        fre_tfidfsum=docid_freq_tfidf[docid]\n",
    "                        fre_tfidfsum[0]+=1\n",
    "                        fre_tfidfsum[1]+=tfidf\n",
    "                        docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        top_ten=gettopten(docid_freq_tfidf)\n",
    "        top_titles=gettitles(top_ten)\n",
    "#         print(top_ten)\n",
    "#         print(top_titles)\n",
    "\n",
    "    else:\n",
    "        st=reg1.sub(' ',st)\n",
    "        st=reg2.sub(' ',st)\n",
    "        st=reg3.sub(' ',st)\n",
    "        st=reg4.sub(' ',st)\n",
    "        st=reg5.sub(' ',st)\n",
    "        st=reg6.sub(' ',st)\n",
    "        st=st.strip()\n",
    "        st=st.split()\n",
    "\n",
    "        st=[word for word in st if word not in stop_words]\n",
    "        st=stemmer.stemWords(st)\n",
    "\n",
    "        for word in st:\n",
    "#             print(word)\n",
    "            postings=getposting(index_path,word)\n",
    "    #         if word.isnumeric():\n",
    "    #             print(postings)\n",
    "            if postings==\"\":\n",
    "                continue\n",
    "\n",
    "\n",
    "            postings=postings[:-1]  #remove new line\n",
    "\n",
    "            postings=postings.split('#')\n",
    "            postings=postings[1:] \n",
    "\n",
    "            for posting in postings:\n",
    "    #             if field=='c':\n",
    "    #                 postinglist=re.findall(r'c[0-9]*',posting,re.DOTALL)\n",
    "    #             if field=='i':\n",
    "    #                 postinglist=re.findall(r'i[0-9]+',posting,re.DOTALL)\n",
    "    #             if field=='t':\n",
    "    #                 postinglist=re.findall(r't[0-9]*',posting,re.DOTALL)\n",
    "    #             if field=='e':\n",
    "    #                 postinglist=re.findall(r'e[0-9]*',posting,re.DOTALL)\n",
    "    #             if field=='b':\n",
    "    #                 postinglist=re.findall(r'b[0-9]*',posting,re.DOTALL)\n",
    "    #             if field=='r':\n",
    "    #                 postinglist=re.findall(r'r[0-9]*',posting,re.DOTALL)\n",
    "\n",
    "                poslist=posting.split('@')\n",
    "                tfidf=int(poslist[1])-10\n",
    "                prepost=poslist[0]\n",
    "\n",
    "\n",
    "                docid=re.findall(r'id[0-9]*',posting,re.DOTALL)\n",
    "                docid=int(re.findall(r'[0-9]+',docid[0],re.DOTALL)[0])\n",
    "\n",
    "                if docid not in docid_freq_tfidf:\n",
    "                    fre_tfidfsum=[1,tfidf]\n",
    "                    docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "                else:\n",
    "                    fre_tfidfsum=docid_freq_tfidf[docid]\n",
    "                    fre_tfidfsum[0]+=1\n",
    "                    fre_tfidfsum[1]+=tfidf\n",
    "                    docid_freq_tfidf[docid]=fre_tfidfsum\n",
    "\n",
    "        top_ten=gettopten(docid_freq_tfidf)\n",
    "\n",
    "        top_titles=gettitles(top_ten)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return (top_ten,top_titles)\n",
    "            \n",
    "            \n",
    "def main():\n",
    "\n",
    "    query_in=open(\"queries.txt\",'r')\n",
    "    query_out=open(\"queries_op.txt\",'w')\n",
    "\n",
    "    for line in query_in.readlines():\n",
    "        print(line)\n",
    "        line=line.split(',')\n",
    "        top_k=int(line[0])\n",
    "        query=line[1][:-1]\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        ids_titles=top_search1(query)         #main search logic return tuple of ids and titles list\n",
    "        print(ids_titles,\"##########################################################3\")\n",
    "        end = timeit.default_timer()\n",
    "\n",
    "        for n in range(top_k):\n",
    "            if n >= len(ids_titles[0]):       #if ids_titles doesn't have the number of documents we are asking for\n",
    "                break\n",
    "            write_line=str(ids_titles[0][n])+\",\"+str(ids_titles[1][n])+\"\\n\"\n",
    "            query_out.write(write_line)\n",
    "\n",
    "        t=end-start\n",
    "        query_out.write(str(round(t,3))+\",\"+str(round(t/top_k,3))+\"\\n\")\n",
    "        query_out.write(\"\\n\")\n",
    "\n",
    "    query_out.close()\n",
    "    print(\"End of processing\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\tmain() \n",
    "        \n",
    "        \n",
    "#     query_out.write(\"\\n\")\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "           \n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "# else:\n",
    "#     st=st.strip()\n",
    "#     tokens=st.split()\n",
    "#     tokens=stemmer.stemWords(tokens)\n",
    "#     fp=open('demo.txt','r')\n",
    "#     lines=fp.readlines()\n",
    "#     for line in lines:\n",
    "#         idx=line.find('#')\n",
    "#         word=line[0:idx]\n",
    "#         if word in tokens:\n",
    "#             print(line)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
